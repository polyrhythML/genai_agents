{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9082dffb-6e8d-46db-ad6c-98d1c341a087",
   "metadata": {},
   "source": [
    "# Lang Chain Experimentation Tutorials HandsOn\n",
    "\n",
    "# [Cookbook for reference](https://www.pinecone.io/learn/series/langchain/)\n",
    "# [Documentation](https://python.langchain.com/docs/get_started/introduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7359705f-c51b-4a0d-9c45-5151503889eb",
   "metadata": {},
   "source": [
    "---\n",
    "Chat with History - ChatGPT API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a8356e10-7b9c-4888-b764-20de76f5fa6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    #!/usr/bin/env python\n",
    "    \"\"\"Example LangChain server exposes and agent that has conversation history.\n",
    "    \n",
    "    In this example, the history is stored entirely on the client's side.\n",
    "    \n",
    "    Please see other examples in LangServe on how to use RunnableWithHistory to\n",
    "    store history on the server side.\n",
    "    \n",
    "    Relevant LangChain documentation:\n",
    "    \n",
    "    * Creating a custom agent: https://python.langchain.com/docs/modules/agents/how_to/custom_agent\n",
    "    * Streaming with agents: https://python.langchain.com/docs/modules/agents/how_to/streaming#custom-streaming-with-events\n",
    "    * General streaming documentation: https://python.langchain.com/docs/expression_language/streaming\n",
    "    * Message History: https://python.langchain.com/docs/expression_language/how_to/message_history\n",
    "    \n",
    "    **ATTENTION**\n",
    "    1. To support streaming individual tokens you will need to use the astream events\n",
    "       endpoint rather than the streaming endpoint.\n",
    "    2. This example does not truncate message history, so it will crash if you\n",
    "       send too many messages (exceed token length).\n",
    "    3. The playground at the moment does not render agent output well! If you want to\n",
    "       use the playground you need to customize it's output server side using astream\n",
    "       events by wrapping it within another runnable.\n",
    "    4. See the client notebook it has an example of how to use stream_events client side!\n",
    "    \"\"\"  # noqa: E501\n",
    "    \n",
    "    \n",
    "    from typing import Any, List, Union\n",
    "    \n",
    "    from fastapi import FastAPI\n",
    "    from langchain.agents import AgentExecutor, tool\n",
    "    from langchain.agents.format_scratchpad.openai_tools import (\n",
    "        format_to_openai_tool_messages,\n",
    "    )\n",
    "    from langchain.agents import Tool, AgentType, initialize_agent\n",
    "    from langchain_community.llms import LlamaCpp\n",
    "    from langchain.agents.output_parsers.openai_tools import OpenAIToolsAgentOutputParser\n",
    "    from langchain.prompts import MessagesPlaceholder\n",
    "    from langchain_community.tools.convert_to_openai import format_tool_to_openai_tool\n",
    "    from langchain_core.messages import AIMessage, FunctionMessage, HumanMessage\n",
    "    from langchain_core.prompts import ChatPromptTemplate\n",
    "    from langchain.utilities import DuckDuckGoSearchAPIWrapper\n",
    "    from langchain_openai import ChatOpenAI\n",
    "    \n",
    "    from langserve import add_routes\n",
    "    from langserve.pydantic_v1 import BaseModel, Field\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"You are a very powerful assistant \"\n",
    "                \"Talk with the user as normal. \"\n",
    "            ),\n",
    "            # Please note the ordering of the fields in the prompt!\n",
    "            # The correct ordering is:\n",
    "            # 1. history - the past messages between the user and the agent\n",
    "            # 2. user - the user's current input\n",
    "            # 3. agent_scratchpad - the agent's working space for thinking and\n",
    "            #    invoking tools to respond to the user's input.\n",
    "            # If you change the ordering, the agent will not work correctly since\n",
    "            # the messages will be shown to the underlying LLM in the wrong order.\n",
    "            \n",
    "            MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "            (\"user\", \"{input}\"),\n",
    "            MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # DuckDuckGoSearchAPIWrapper does not require an API key\n",
    "    # search = DuckDuckGoSearchAPIWrapper()\n",
    "    # search_tool = Tool(name=\"Current Search\",\n",
    "    #                    func=search.run,\n",
    "    #                    description=\"Useful when you need to answer questions about nouns, current events or the current state of the world.\"\n",
    "    #                    )\n",
    "    # tools = [search_tool]\n",
    "    \n",
    "    \n",
    "    tools = [word_length]\n",
    "    llm_with_tools = llm.bind(tools=[format_tool_to_openai_tool(tool) for tool in tools])\n",
    "    \n",
    "    \n",
    "    \n",
    "    # We need to set streaming=True on the LLM to support streaming individual tokens.\n",
    "    # Tokens will be available when using the stream_log / stream events endpoints,\n",
    "    # but not when using the stream endpoint since the stream implementation for agent\n",
    "    # streams action observation pairs not individual tokens.\n",
    "    # See the client notebook that shows how to use the stream events endpoint.\n",
    "    \n",
    "    # Initialize LlamaCpp with your model's parameters\n",
    "    # llm = LlamaCpp(\n",
    "    #     model_path=\"../models/mixtral-8x7b-instruct-v0.1.Q5_K_M.gguf\",\n",
    "    #     n_gpu_layers=1,\n",
    "    #     n_batch=512,\n",
    "    #     f16_kv=True,\n",
    "    #     verbose=True,\n",
    "    #     streaming=True\n",
    "    # )\n",
    "    from openai import OpenAI\n",
    "    \n",
    "    llm = ChatOpenAI(\n",
    "        model=\"gpt-3.5-turbo\", \n",
    "        temperature=0, \n",
    "        streaming=True, \n",
    "        api_key=\"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\"\n",
    "    )\n",
    "    \n",
    "    # ATTENTION: For production use case, it's a good idea to trim the prompt to avoid\n",
    "    #            exceeding the context window length used by the model.\n",
    "    #\n",
    "    # To fix that simply adjust the chain to trim the prompt in whatever way\n",
    "    # is appropriate for your use case.\n",
    "    # For example, you may want to keep the system message and the last 10 messages.\n",
    "    # Or you may want to trim based on the number of tokens.\n",
    "    # Or you may want to also summarize the messages to keep information about things\n",
    "    # that were learned about the user.\n",
    "    #\n",
    "    # def prompt_trimmer(messages: List[Union[HumanMessage, AIMessage, FunctionMessage]]):\n",
    "    #     '''Trims the prompt to a reasonable length.'''\n",
    "    #     # Keep in mind that when trimming you may want to keep the system message!\n",
    "    #     return messages[-10:] # Keep last 10 messages.\n",
    "    \n",
    "    agent = (\n",
    "        {\n",
    "            \"input\": lambda x: x[\"input\"],\n",
    "            \"agent_scratchpad\": lambda x: format_to_openai_tool_messages(\n",
    "                x[\"intermediate_steps\"]\n",
    "            ),\n",
    "            \"chat_history\": lambda x: x[\"chat_history\"],\n",
    "        }\n",
    "        | prompt\n",
    "        # | prompt_trimmer # See comment above.\n",
    "        | llm_with_tools\n",
    "        | OpenAIToolsAgentOutputParser()\n",
    "    )\n",
    "\n",
    "    agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "    \n",
    "    app = FastAPI(\n",
    "        title=\"LangChain Server\",\n",
    "        version=\"1.0\",\n",
    "        description=\"Spin up a simple api server using LangChain's Runnable interfaces\",\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # We need to add these input/output schemas because the current AgentExecutor\n",
    "    # is lacking in schemas.\n",
    "    class Input(BaseModel):\n",
    "        input: str\n",
    "        # The field extra defines a chat widget.\n",
    "        # Please see documentation about widgets in the main README.\n",
    "        # The widget is used in the playground.\n",
    "        # Keep in mind that playground support for agents is not great at the moment.\n",
    "        # To get a better experience, you'll need to customize the streaming output\n",
    "        # for now.\n",
    "        chat_history: List[Union[HumanMessage, AIMessage, FunctionMessage]] = Field(\n",
    "            ...,\n",
    "            extra={\"widget\": {\"type\": \"chat\", \"input\": \"input\", \"output\": \"output\"}},\n",
    "        )\n",
    "    \n",
    "    \n",
    "    class Output(BaseModel):\n",
    "        output: Any\n",
    "    \n",
    "    \n",
    "    # Adds routes to the app for using the chain under:\n",
    "    # /invoke\n",
    "    # /batch\n",
    "    # /stream\n",
    "    # /stream_events\n",
    "    add_routes(\n",
    "        app,\n",
    "        agent_executor.with_types(input_type=Input, output_type=Output).with_config(\n",
    "            {\"run_name\": \"agent\"}\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    if __name__ == \"__main__\":\n",
    "        import uvicorn\n",
    "        uvicorn.run(app, host=\"localhost\", port=8000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0e3dfa-bf5d-4c2b-8336-68af3f2e72a7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Langserve + History "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6e86db7d-ea45-4bda-a2bc-6087ed3f1ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.agents import Tool, AgentType, initialize_agent\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.utilities import DuckDuckGoSearchAPIWrapper\n",
    "from langchain.agents import AgentExecutor\n",
    "from fastapi import FastAPI\n",
    "from langchain_core.messages import AIMessage, FunctionMessage, HumanMessage\n",
    "from typing import Any, List, Union\n",
    "from langserve import add_routes\n",
    "\n",
    "# Adding a search tool\n",
    "search = DuckDuckGoSearchAPIWrapper()\n",
    "search_tool = Tool(name=\"Current Search\",\n",
    "                   func=search.run,\n",
    "                   description=\"Useful when you need to answer questions about nouns, current events or the current state of the world.\"\n",
    "                   )\n",
    "\n",
    "tools = [search_tool]\n",
    "\n",
    "# Memory Buffer Added\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
    "\n",
    "\n",
    "# Llamacpp Model added\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"../models/mixtral-8x7b-instruct-v0.1.Q5_K_M.gguf\",\n",
    "    n_gpu_layers=1,\n",
    "    n_batch=512,\n",
    "    n_ctx=2048,\n",
    "    f16_kv=True,\n",
    "    verbose=True,\n",
    "    streaming=True\n",
    ")\n",
    "\n",
    "# Agent Executor Initialized\n",
    "agent_executor = AgentExecutor(agent=agent_chain, tools=tools, verbose=True)\n",
    "\n",
    "# FastAPI app executor\n",
    "app = FastAPI(\n",
    "    title=\"LangChain Server\",\n",
    "    version=\"1.0\",\n",
    "    description=\"Spin up a simple api server using LangChain's Runnable interfaces\",\n",
    ")\n",
    "\n",
    "\n",
    "class Input(BaseModel):\n",
    "    input: str\n",
    "    # The field extra defines a chat widget.\n",
    "    # Please see documentation about widgets in the main README.\n",
    "    # The widget is used in the playground.\n",
    "    # Keep in mind that playground support for agents is not great at the moment.\n",
    "    # To get a better experience, you'll need to customize the streaming output\n",
    "    # for now.\n",
    "    chat_history: List[Union[HumanMessage, AIMessage, FunctionMessage]] = Field(\n",
    "        ...,\n",
    "        extra={\"widget\": {\"type\": \"chat\", \"input\": \"input\", \"output\": \"output\"}},\n",
    "    )\n",
    "\n",
    "\n",
    "# Add application route\n",
    "add_routes(\n",
    "    app,\n",
    "    agent_executor.with_types(input_type=Input, output_type=str).with_config(\n",
    "        {\"run_name\": \"agent\"}\n",
    "    ),\n",
    ")\n",
    "\n",
    "#agent_chain.run(input=\"What it do, nephew!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"localhost\", port=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5cf93711-c47d-4fc1-add7-a5f87038b12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from langserve import RemoteRunnable\n",
    "from langchain_core.messages import AIMessage, FunctionMessage, HumanMessage\n",
    "\n",
    "chat = RemoteRunnable(\"http://localhost:8000/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55489db9-7317-48e7-a828-bcea0607407e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Conversational Agent - Simple Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "788523a3-7927-43a6-b7ab-42c29d32874e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from pydantic import BaseModel, Field\n",
    "import datetime\n",
    "import wikipedia\n",
    "from langchain.utilities import DuckDuckGoSearchAPIWrapper\n",
    "from langchain.agents import Tool\n",
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def search_wikipedia(query: str) -> str:\n",
    "    \"\"\"Run Wikipedia search and get page summaries.\"\"\"\n",
    "    page_titles = wikipedia.search(query)\n",
    "    summaries = []\n",
    "    for page_title in page_titles[: 3]:\n",
    "        try:\n",
    "            wiki_page =  wikipedia.page(title=page_title, auto_suggest=False)\n",
    "            summaries.append(f\"Page: {page_title}\\nSummary: {wiki_page.summary}\")\n",
    "        except (\n",
    "            self.wiki_client.exceptions.PageError,\n",
    "            self.wiki_client.exceptions.DisambiguationError,\n",
    "        ):\n",
    "            pass\n",
    "    if not summaries:\n",
    "        return \"No good Wikipedia Search Result was found\"\n",
    "    return \"\\n\\n\".join(summaries)\n",
    "\n",
    "# Adding a search tool\n",
    "search = DuckDuckGoSearchAPIWrapper()\n",
    "search_tool = Tool(name=\"Current Search\",\n",
    "                   func=search.run,\n",
    "                   description=\"Useful when you need to answer questions about nouns, current events or the current state of the world.\"\n",
    "                   )\n",
    "\n",
    "tools = [search_tool, search_wikipedia]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8cb8b743-5fd8-4350-abc1-fb1c25398e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "from langchain.tools.render import format_tool_to_openai_function\n",
    "from langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser\n",
    "from langchain_community.llms import LlamaCpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b7cfa2-7d6d-46d6-8e81-cb965c1250a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 26 key-value pairs and 995 tensors from ../models/mixtral-8x7b-instruct-v0.1.Q5_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mixtral-8x7b-instruct-v0.1\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:                         llama.expert_count u32              = 8\n",
      "llama_model_loader: - kv  10:                    llama.expert_used_count u32              = 2\n",
      "llama_model_loader: - kv  11:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  12:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  13:                          general.file_type u32              = 17\n",
      "llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  25:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type  f16:   32 tensors\n",
      "llama_model_loader: - type q8_0:   64 tensors\n",
      "llama_model_loader: - type q5_K:  833 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 8\n",
      "llm_load_print_meta: n_expert_used    = 2\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q5_K - Medium\n",
      "llm_load_print_meta: model params     = 46.70 B\n",
      "llm_load_print_meta: model size       = 30.02 GiB (5.52 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mixtral-8x7b-instruct-v0.1\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.38 MiB\n"
     ]
    }
   ],
   "source": [
    "functions = [format_tool_to_openai_function(f) for f in tools]\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are helpful but sassy assistant\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "])\n",
    "\n",
    "# Llamacpp Model added\n",
    "model = LlamaCpp(\n",
    "    model_path=\"../models/mixtral-8x7b-instruct-v0.1.Q5_K_M.gguf\",\n",
    "    n_gpu_layers=1,\n",
    "    n_batch=512,\n",
    "    n_ctx=2048,\n",
    "    f16_kv=True,\n",
    "    verbose=True,\n",
    "    streaming=True\n",
    ")\n",
    "# Chaining \n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9e3c7e39-d7c9-4f1f-bea8-ae9c93495e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    1362.06 ms\n",
      "llama_print_timings:      sample time =      28.00 ms /   158 runs   (    0.18 ms per token,  5642.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1362.01 ms /    21 tokens (   64.86 ms per token,    15.42 tokens per second)\n",
      "llama_print_timings:        eval time =   19156.39 ms /   157 runs   (  122.02 ms per token,     8.20 tokens per second)\n",
      "llama_print_timings:       total time =   20899.83 ms /   178 tokens\n"
     ]
    }
   ],
   "source": [
    "result = chain.invoke({\"input\": \"what is the weather is sf?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47f87239-bc58-4477-9f5c-194586e72762",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1355.52 ms\n",
      "llama_print_timings:      sample time =      10.62 ms /    58 runs   (    0.18 ms per token,  5459.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =     635.20 ms /     8 tokens (   79.40 ms per token,    12.59 tokens per second)\n",
      "llama_print_timings:        eval time =    6933.23 ms /    57 runs   (  121.64 ms per token,     8.22 tokens per second)\n",
      "llama_print_timings:       total time =    7710.57 ms /    65 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nAssistant: Hello there, as of my last update, the Indian Premier League (IPL) schedule for today hasn't been released yet. I recommend checking the official IPL website or other reliable sports sources for the most current and accurate information. Have a great day!\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"which teams are playing IPL today ?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cca6142-fabb-4043-987f-0803a4e99192",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Conversational Agent - Memory + Tools + AgentFinisher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfd92843-e6e3-4e82-9d5a-716c7e83210e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import MessagesPlaceholder\n",
    "from pydantic import BaseModel, Field\n",
    "import datetime\n",
    "import wikipedia\n",
    "from langchain.utilities import DuckDuckGoSearchAPIWrapper\n",
    "from langchain.agents import Tool\n",
    "from langchain.tools import tool\n",
    "from langchain.tools.render import format_tool_to_openai_function\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser\n",
    "from langchain.utilities import DuckDuckGoSearchAPIWrapper\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain.agents.format_scratchpad import format_to_openai_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "759f9d36-0176-44f7-a5c0-97fbc9fc7d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Wikipedia tool for searching specific query\n",
    "\n",
    "import wikipedia\n",
    "\n",
    "@tool\n",
    "def search_wikipedia(query: str) -> str:\n",
    "    \"\"\"Run Wikipedia search and get page summaries.\"\"\"\n",
    "    page_titles = wikipedia.search(query)\n",
    "    summaries = []\n",
    "    for page_title in page_titles[: 3]:\n",
    "        try:\n",
    "            wiki_page =  wikipedia.page(title=page_title, auto_suggest=False)\n",
    "            summaries.append(f\"Page: {page_title}\\nSummary: {wiki_page.summary}\")\n",
    "        except (\n",
    "            self.wiki_client.exceptions.PageError,\n",
    "            self.wiki_client.exceptions.DisambiguationError,\n",
    "        ):\n",
    "            pass\n",
    "    if not summaries:\n",
    "        return \"No good Wikipedia Search Result was found\"\n",
    "    return \"\\n\\n\".join(summaries)\n",
    "\n",
    "\n",
    "# Use Duck Duck Go for searching web for real-time queries, e.g. current affairs.\n",
    "# Adding a search tool\n",
    "\n",
    "search = DuckDuckGoSearchAPIWrapper()\n",
    "search_tool = Tool(name=\"Current Search\",\n",
    "                   func=search.run,\n",
    "                   description=\"Useful when you need to answer questions about nouns, current events or the current state of the world.\"\n",
    "                   )\n",
    "\n",
    "tools = [search_tool, search_wikipedia]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f0affcd-59b0-43ea-877c-24ced61039a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/opt/miniconda/envs/amit_env/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `format_tool_to_openai_function` was deprecated in LangChain 0.1.16 and will be removed in 0.2.0. Use langchain_core.utils.function_calling.convert_to_openai_function() instead.\n",
      "  warn_deprecated(\n",
      "/data/opt/miniconda/envs/amit_env/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import MessagesPlaceholder\n",
    "from pydantic import BaseModel, Field\n",
    "import datetime\n",
    "import wikipedia\n",
    "from langchain.utilities import DuckDuckGoSearchAPIWrapper\n",
    "from langchain.agents import Tool\n",
    "from langchain.tools import tool\n",
    "from langchain.tools.render import format_tool_to_openai_function\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser\n",
    "from langchain.utilities import DuckDuckGoSearchAPIWrapper\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain.agents.format_scratchpad import format_to_openai_functions\n",
    "from langchain_community.llms import LlamaCpp\n",
    "\n",
    "functions = [format_tool_to_openai_function(f) for f in tools]\n",
    "\n",
    "# Binding tools functions with OpenAI model\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\", \n",
    "                   temperature=0, \n",
    "                   streaming=True, \n",
    "                   api_key=\"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\")\n",
    "\n",
    "\n",
    "\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"../models/mixtral-8x7b-instruct-v0.1.Q5_K_M.gguf\",\n",
    "    n_gpu_layers=1,\n",
    "    n_batch=512,\n",
    "    f16_kv=True,\n",
    "    verbose=True,\n",
    "    streaming=True\n",
    ")\n",
    "\n",
    "# Adding ConversationBuffer\n",
    "memory = ConversationBufferMemory(return_messages=True,memory_key=\"chat_history\")\n",
    "\n",
    "# Defining Chatprompt\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are helpful but sassy assistant\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "    MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n",
    "])\n",
    "\n",
    "# Agent Scratchpad is the intermediate thinking the Agent does\n",
    "# Runnable Passthrough provides intermediate inputs to the conversation\n",
    "chain = RunnablePassthrough.assign(\n",
    "    agent_scratchpad = lambda x: format_to_openai_functions(x[\"intermediate_steps\"])\n",
    ") | prompt | model | OpenAIFunctionsAgentOutputParser()\n",
    "\n",
    "# AgentExecutor takes in lang chain\n",
    "qa = AgentExecutor(agent=chain, tools=tools, verbose=True, memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b601a2c2-5930-4298-b4a4-6438f4728294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI'm here and sassy as ever, ready to assist you with anything you need. How can I help you today?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I'm here and sassy as ever, ready to assist you with anything you need. How can I help you today?\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = qa.invoke({\"input\": \"How are you doing today ?\"})\n",
    "answer = result['output'] \n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefd00ce-e872-43f2-a88a-411629613f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, Body\n",
    "from pydantic import BaseModel\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import MessagesPlaceholder\n",
    "from langchain.agents import Tool\n",
    "from langchain.tools import tool\n",
    "from langchain.tools.render import format_tool_to_openai_function\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain.agents.format_scratchpad import format_to_openai_functions\n",
    "from langchain_community.llms import LlamaCpp\n",
    "\n",
    "# Define the request body model\n",
    "class QAInput(BaseModel):\n",
    "    input: str\n",
    "\n",
    "# Define the response model\n",
    "class QAResponse(BaseModel):\n",
    "    output: str\n",
    "\n",
    "# Initialize FastAPI app\n",
    "app = FastAPI()\n",
    "\n",
    "# model = LlamaCpp(\n",
    "#     model_path=\"../models/mixtral-8x7b-instruct-v0.1.Q5_K_M.gguf\",\n",
    "#     n_gpu_layers=1,\n",
    "#     n_batch=512,\n",
    "#     f16_kv=True,\n",
    "#     verbose=True,\n",
    "#     streaming=True\n",
    "# )\n",
    "\n",
    "# Binding tools functions with OpenAI model\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\", \n",
    "                temperature=0, \n",
    "                streaming=True, \n",
    "                api_key=\"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\")\n",
    "\n",
    "def create_qa_agent():\n",
    "    # Your existing code here...\n",
    "    # ...\n",
    "    functions = [format_tool_to_openai_function(f) for f in tools]\n",
    "\n",
    "    # Adding ConversationBuffer\n",
    "    memory = ConversationBufferMemory(return_messages=True,memory_key=\"chat_history\")\n",
    "\n",
    "    # Defining Chatprompt\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are a Helpful Chat Assistant who has a formal tone to answer the questions.\"),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"user\", \"{input}\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n",
    "    ])\n",
    "\n",
    "    # Agent Scratchpad is the intermediate thinking the Agent does\n",
    "    # Runnable Passthrough provides intermediate inputs to the conversation\n",
    "    chain = RunnablePassthrough.assign(\n",
    "        agent_scratchpad = lambda x: format_to_openai_functions(x[\"intermediate_steps\"])\n",
    "    ) | prompt | model #| OpenAIFunctionsAgentOutputParser()\n",
    "\n",
    "    # AgentExecutor takes in lang chain\n",
    "    qa = AgentExecutor(agent=chain, tools=tools, verbose=True, memory=memory)\n",
    "    return qa\n",
    "\n",
    "# Store the qa agent in a global variable so it's not recreated every time the endpoint is called\n",
    "qa_agent = create_qa_agent()\n",
    "\n",
    "@app.put(\"/ask\", response_model=QAResponse)\n",
    "async def ask_question(input: QAInput = Body(...)):\n",
    "    result = qa_agent.invoke({\"input\": input.input})\n",
    "    answer = result['output']\n",
    "    return QAResponse(output=answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21aeca49-a528-414c-b66c-ef0250e0188f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Client Code \n",
    "\n",
    "import requests\n",
    "\n",
    "def chat_with_agent():\n",
    "    base_url = \"http://localhost:8000\"  # Replace with your server's address if not running locally\n",
    "    stop_word = \"stop\"  # The word to stop the conversation\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() == stop_word:\n",
    "            print(\"Ending conversation.\")\n",
    "            break\n",
    "\n",
    "        response = requests.put(f\"{base_url}/ask\", json={\"input\": user_input})\n",
    "        response.raise_for_status()  # Raise an exception if the request failed\n",
    "\n",
    "        agent_output = response.json()[\"output\"]\n",
    "        print(f\"Agent: {agent_output}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa926ba1-16f9-48fa-859f-05c074da9c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  Hi wassup\n"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m/data/opt/miniconda/envs/amit_env/lib/python3.10/site-packages/requests/models.py:971\u001b[0m, in \u001b[0;36mResponse.json\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    970\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 971\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcomplexjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    973\u001b[0m     \u001b[38;5;66;03m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001b[39;00m\n\u001b[1;32m    974\u001b[0m     \u001b[38;5;66;03m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[39;00m\n",
      "File \u001b[0;32m/data/opt/miniconda/envs/amit_env/lib/python3.10/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/data/opt/miniconda/envs/amit_env/lib/python3.10/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03mcontaining a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n",
      "File \u001b[0;32m/data/opt/miniconda/envs/amit_env/lib/python3.10/json/decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mchat_with_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 28\u001b[0m, in \u001b[0;36mchat_with_agent\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28mprint\u001b[39m(response)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m#response = requests.put(f\"{base_url}/ask\", json={\"input\": user_input})\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m#response.raise_for_status()  # Raise an exception if the request failed\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m agent_output \u001b[38;5;241m=\u001b[39m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAgent: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00magent_output\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/data/opt/miniconda/envs/amit_env/lib/python3.10/site-packages/requests/models.py:975\u001b[0m, in \u001b[0;36mResponse.json\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    971\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m complexjson\u001b[38;5;241m.\u001b[39mloads(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    972\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    973\u001b[0m     \u001b[38;5;66;03m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001b[39;00m\n\u001b[1;32m    974\u001b[0m     \u001b[38;5;66;03m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[39;00m\n\u001b[0;32m--> 975\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RequestsJSONDecodeError(e\u001b[38;5;241m.\u001b[39mmsg, e\u001b[38;5;241m.\u001b[39mdoc, e\u001b[38;5;241m.\u001b[39mpos)\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "chat_with_agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf64292-f69a-4822-a687-56002db4e743",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
